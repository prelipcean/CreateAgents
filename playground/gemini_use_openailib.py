"""
This script demonstrates how to evaluate the intelligence of different
Large Language Models (LLMs) by asking them a challenging, nuanced question.
It uses the OpenAI Python client library to interact with Google Gemini models
("gemini-1.5-flash" and "gemini-2.0-flash") via their OpenAI-compatible endpoint.

The script performs the following steps:
1. Loads the Google API key from environment variables for secure access.
2. Initializes the OpenAI client, pointing it to the Gemini API.
3. Crafts a specific request to generate a complex evaluation question.
4. Uses "gemini-1.5-flash" to generate the challenging question.
5. Uses "gemini-2.0-flash" to answer the question generated by the first model,
   allowing for a comparison of their capabilities.
"""

import os
from dotenv import load_dotenv
from openai import OpenAI

# --- Configuration and API Key Loading ---

# Load environment variables from a .env file.
# The 'override=True' argument ensures that existing environment variables
# are overwritten if a .env file provides new values. This is good practice
# for development environments.
load_dotenv(override=True)

# Retrieve the Google API key from environment variables.
# This key is essential for authenticating with the Gemini API.
google_api_key = os.getenv("GOOGLE_API_KEY")

# Check if the API key is successfully loaded.
# It's crucial to confirm the key is present before attempting API calls.
if google_api_key:
    # Print only the first few characters of the key for security,
    # to confirm it's loaded without exposing the full key in logs.
    print(f"Google API Key exists and begins {google_api_key[:8]}********")
else:
    # Inform the user if the API key is not set, as it's required for this script.
    # Note: The original comment "and this is optional" was incorrect,
    # as the key is indeed necessary for API interaction.
    print(
        "Error: Google API Key not set. Please ensure 'GOOGLE_API_KEY' is in your .env file or environment variables."
    )
    exit()  # Exit the script if the API key is missing to prevent errors.

# --- Define Request for Challenging Question ---

# Craft a detailed request for the LLM to generate a challenging evaluation question.
# The instruction "Answer only with the question, no explanation" is important
# for controlling the format of the LLM's response.
request_for_question = (
    "Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. "
    "Answer only with the question, no explanation."
)

# Prepare the message in the OpenAI chat completions format.
# A "user" role is used for the prompt we are sending to the model.
messages_for_question_generation = [{"role": "user", "content": request_for_question}]

# --- Initialize OpenAI Client for Gemini (for question generation) ---

# Initialize the OpenAI client.
# The `api_key` is your Google API key.
# The `base_url` points to Google's OpenAI-compatible endpoint for Gemini models.
# This client instance will be used to get the challenging question.
gemini_client_for_question = OpenAI(
    api_key=google_api_key,
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
)

# Specify the model for generating the question.
# "gemini-1.5-flash" is a good choice for fast, general-purpose text generation.
model_for_question_generation = "gemini-1.5-flash"

# --- Generate the Challenging Question ---

print(
    f"\n--- Generating challenging question using {model_for_question_generation} ---"
)

# Make the API call to generate the question.
# The `messages` list provides the conversational context (in this case, just the prompt).
response_question = gemini_client_for_question.chat.completions.create(
    model=model_for_question_generation,
    messages=messages_for_question_generation,
    temperature=0.7,  # A slightly higher temperature for more creative question generation
    max_tokens=200,  # Limit the length of the generated question
)

# Extract the generated question from the response.
# Accessing `response.choices[0].message.content` is the standard way for OpenAI API.
generated_question = response_question.choices[0].message.content
print("Generated Question:\n" + generated_question)

# --- Initialize OpenAI Client for Gemini (for answering the question) ---

# Initialize a new OpenAI client instance.
# While you could reuse the previous `gemini` object, creating a new one
# explicitly for the second task (with a potentially different model or config)
# makes the code clearer and more modular.
gemini_client_for_answer = OpenAI(
    api_key=google_api_key,
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
)

# Specify the model for answering the question.
# "gemini-2.0-flash" is a newer, potentially more capable model than 1.5-flash,
# making it a good candidate for answering a "challenging" question.
model_for_answer = "gemini-2.0-flash"

# Prepare the messages list for asking the generated question.
# The `generated_question` now becomes the user's prompt for the second model.
messages_for_answer = [{"role": "user", "content": generated_question}]

# --- Get the Answer to the Challenging Question ---

print(f"\n--- Getting answer using {model_for_answer} ---")

# Make the API call to get the answer.
response_answer = gemini_client_for_answer.chat.completions.create(
    model=model_for_answer,
    messages=messages_for_answer,
    temperature=0.5,  # A lower temperature for a more direct and less creative answer
    max_tokens=500,  # Allow for a more comprehensive answer
    top_p=0.9,
    # You can add other parameters here if desired, e.g.,
    # top_k=40, # Note: top_k might not be directly exposed in OpenAI client for Gemini,
    #           # or might require `extra_body` for specific implementations.
    #           # It's safer to stick to standard OpenAI params first.
    # n=1, # Number of completions to generate. Default is 1.
    # stop=["\n\nEND"], # Stop generation if these sequences are encountered.
)

# Extract the answer from the response.
answer_content = response_answer.choices[0].message.content
print("Answer from " + model_for_answer + ":\n" + answer_content)
